{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Programming Exercise 6:\n# Support Vector Machines\n\n## OPTIONAL EXERCISE",
   "metadata": {
    "cell_id": "00000-a3400fa5-b6ef-4207-b8a2-70c6771bdad3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "897a9f0",
    "execution_start": 1630635706671,
    "execution_millis": 2309,
    "cell_id": "00001-4522c5d9-8e40-4b9f-8a47-534297cd8a63",
    "deepnote_cell_type": "code"
   },
   "source": "# used for manipulating directory paths\nimport os\n\n# Scientific and vector computation for python\nimport numpy as np\n\n# Import regular expressions to process emails\nimport re\n\n# Plotting library\nfrom matplotlib import pyplot\n\n# Optimization module in scipy\nfrom scipy import optimize\n\n# will be used to load MATLAB mat datafile format\nfrom scipy.io import loadmat\n\n# library written for this exercise providing additional functions for assignment submission, and others\nimport utils\n\n# define the submission/grader object for this exercise\ngrader = utils.Grader()\n\n# tells matplotlib to embed plots within the notebook\n%matplotlib inline",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e194273a",
    "execution_start": 1630635708989,
    "execution_millis": 6,
    "cell_id": "00029-490b4033-3542-4fd5-853a-49f9f84312c5",
    "deepnote_cell_type": "code"
   },
   "source": "def processEmail(email_contents, verbose=True):\n    \"\"\"\n    Preprocesses the body of an email and returns a list of indices \n    of the words contained in the email.    \n    \n    Parameters\n    ----------\n    email_contents : str\n        A string containing one email. \n    \n    verbose : bool\n        If True, print the resulting email after processing.\n    \n    Returns\n    -------\n    word_indices : list\n        A list of integers containing the index of each word in the \n        email which is also present in the vocabulary.\n    \n    Instructions\n    ------------\n    Fill in this function to add the index of word to word_indices \n    if it is in the vocabulary. At this point of the code, you have \n    a stemmed word from the email in the variable word.\n    You should look up word in the vocabulary list (vocabList). \n    If a match exists, you should add the index of the word to the word_indices\n    list. Concretely, if word = 'action', then you should\n    look up the vocabulary list to find where in vocabList\n    'action' appears. For example, if vocabList[18] =\n    'action', then, you should add 18 to the word_indices \n    vector (e.g., word_indices.append(18)).\n    \n    Notes\n    -----\n    - vocabList[idx] returns a the word with index idx in the vocabulary list.\n    \n    - vocabList.index(word) return index of word `word` in the vocabulary list.\n      (A ValueError exception is raised if the word does not exist.)\n    \"\"\"\n    # Load Vocabulary\n    vocabList = utils.getVocabList()\n\n    # Init return value\n    word_indices = []\n\n    # ========================== Preprocess Email ===========================\n    # Find the Headers ( \\n\\n and remove )\n    # Uncomment the following lines if you are working with raw emails with the\n    # full headers\n    # hdrstart = email_contents.find(chr(10) + chr(10))\n    # email_contents = email_contents[hdrstart:]\n\n    # Lower case\n    email_contents = email_contents.lower()\n    \n    # Strip all HTML\n    # Looks for any expression that starts with < and ends with > and replace\n    # and does not have any < or > in the tag it with a space\n    email_contents =re.compile('<[^<>]+>').sub(' ', email_contents)\n\n    # Handle Numbers\n    # Look for one or more characters between 0-9\n    email_contents = re.compile('[0-9]+').sub(' number ', email_contents)\n\n    # Handle URLS\n    # Look for strings starting with http:// or https://\n    email_contents = re.compile('(http|https)://[^\\s]*').sub(' httpaddr ', email_contents)\n\n    # Handle Email Addresses\n    # Look for strings with @ in the middle\n    email_contents = re.compile('[^\\s]+@[^\\s]+').sub(' emailaddr ', email_contents)\n    \n    # Handle $ sign\n    email_contents = re.compile('[$]+').sub(' dollar ', email_contents)\n    \n    # get rid of any punctuation\n    email_contents = re.split('[ @$/#.-:&*+=\\[\\]?!(){},''\">_<;%\\n\\r]', email_contents)\n\n    # remove any empty word string\n    email_contents = [word for word in email_contents if len(word) > 0]\n    \n    # Stem the email contents word by word\n    stemmer = utils.PorterStemmer()\n    processed_email = []\n    for word in email_contents:\n        # Remove any remaining non alphanumeric characters in word\n        word = re.compile('[^a-zA-Z0-9]').sub('', word).strip()\n        word = stemmer.stem(word)\n        processed_email.append(word)\n\n        if len(word) < 1:\n            continue\n\n        # Look up the word in the dictionary and add to word_indices if found\n        # ====================== YOUR CODE HERE ======================\n        \n        try:\n            idx = vocabList.index(word)\n            word_indices.append(idx)\n        except ValueError:\n            pass\n\n        # =============================================================\n\n    if verbose:\n        print('----------------')\n        print('Processed email:')\n        print('----------------')\n        print(' '.join(processed_email))\n    return word_indices",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3cd50777",
    "execution_start": 1630635709000,
    "execution_millis": 5,
    "cell_id": "00035-dbf3041f-06c5-4aea-b81b-c32af9fc2bf0",
    "deepnote_cell_type": "code"
   },
   "source": "def emailFeatures(word_indices):\n    \"\"\"\n    Takes in a word_indices vector and produces a feature vector from the word indices. \n    \n    Parameters\n    ----------\n    word_indices : list\n        A list of word indices from the vocabulary list.\n    \n    Returns\n    -------\n    x : list \n        The computed feature vector.\n    \n    Instructions\n    ------------\n    Fill in this function to return a feature vector for the\n    given email (word_indices). To help make it easier to  process \n    the emails, we have have already pre-processed each email and converted\n    each word in the email into an index in a fixed dictionary (of 1899 words).\n    The variable `word_indices` contains the list of indices of the words \n    which occur in one email.\n    \n    Concretely, if an email has the text:\n\n        The quick brown fox jumped over the lazy dog.\n\n    Then, the word_indices vector for this text might look  like:\n               \n        60  100   33   44   10     53  60  58   5\n\n    where, we have mapped each word onto a number, for example:\n\n        the   -- 60\n        quick -- 100\n        ...\n\n    Note\n    ----\n    The above numbers are just an example and are not the actual mappings.\n\n    Your task is take one such `word_indices` vector and construct\n    a binary feature vector that indicates whether a particular\n    word occurs in the email. That is, x[i] = 1 when word i\n    is present in the email. Concretely, if the word 'the' (say,\n    index 60) appears in the email, then x[60] = 1. The feature\n    vector should look like:\n        x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..]\n    \"\"\"\n    # Total number of words in the dictionary\n    n = 1899\n\n    # You need to return the following variables correctly.\n    x = np.zeros(n)\n\n    # ===================== YOUR CODE HERE ======================\n    for a in word_indices:\n        x[a] = 1    \n    # ===========================================================\n    \n    return x",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.6 Optional (ungraded) exercise: Build your own dataset\n\nIn this exercise, we provided a preprocessed training set and test set. These datasets were created using the same functions (`processEmail` and `emailFeatures`) that you now have completed. For this optional (ungraded) exercise, you will build your own dataset using the original emails from the SpamAssassin Public Corpus.\n\nYour task in this optional (ungraded) exercise is to download the original\nfiles from the public corpus and extract them. After extracting them, you should run the `processEmail` and `emailFeatures` functions on each email to extract a feature vector from each email. This will allow you to build a dataset `X`, `y` of examples. You should then randomly divide up the dataset into a training set, a cross validation set and a test set.\n\nWhile you are building your own dataset, we also encourage you to try building your own vocabulary list (by selecting the high frequency words that occur in the dataset) and adding any additional features that you think\nmight be useful. Finally, we also suggest trying to use highly optimized SVM toolboxes such as [`LIBSVM`](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) or [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm).\n\n*You do not need to submit any solutions for this optional (ungraded) exercise.*",
   "metadata": {
    "cell_id": "00049-51f7a20d-9c76-486e-a001-712954cec60a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00050-b441ebf7-cb07-4450-84cd-549a0c6ff01c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b0b967ff",
    "execution_start": 1630635709006,
    "execution_millis": 48,
    "deepnote_cell_type": "code"
   },
   "source": "DATASETS_DIR = 'datasets'\nMODELS_DIR = 'models'\nTAR_DIR = os.path.join(DATASETS_DIR, 'tar')\n\nSPAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2'\nEASY_HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2'\nHARD_HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00047-8859d577-598d-4bc6-8b62-346e54bc4852",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1df1f3a0",
    "execution_start": 1630635709054,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "from urllib.request import urlretrieve\nimport tarfile\nimport shutil\n\ndef download_dataset(url):\n    \"\"\"download and unzip data from a url into the specified path\"\"\"\n    \n    # create directory if it doesn't exist\n    if not os.path.isdir(TAR_DIR):\n        os.makedirs(TAR_DIR)\n    \n    filename = url.rsplit('/', 1)[-1]\n    tarpath = os.path.join(TAR_DIR, filename)\n    \n    # download the tar file if it doesn't exist\n    try:\n        tarfile.open(tarpath)\n    except:\n        urlretrieve(url, tarpath)\n    \n    with tarfile.open(tarpath) as tar:\n        dirname = os.path.join(DATASETS_DIR, tar.getnames()[0])\n        if os.path.isdir(dirname):\n            shutil.rmtree(dirname)\n        tar.extractall(path=DATASETS_DIR)\n        \n        cmds_path = os.path.join(dirname, 'cmds')\n        if os.path.isfile(cmds_path):\n            os.remove(cmds_path)\n    \n    return dirname",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00048-ccfae06d-e47c-45a3-b8a1-5df8bd1867e2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e51ae5be",
    "execution_start": 1630635709055,
    "execution_millis": 8060,
    "deepnote_cell_type": "code"
   },
   "source": "spam_dir = download_dataset(SPAM_URL)\neasy_ham_dir = download_dataset(EASY_HAM_URL)\nhard_ham_dir = download_dataset(HARD_HAM_URL)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00049-af02a0c8-305c-4ca9-9667-0870a3ba9b5c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "525a60c5",
    "execution_start": 1630635717122,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "import glob\n\ndef load_dataset(dirpath):\n    \"\"\"load emails from the specified directory\"\"\"\n    \n    files = []\n    filepaths = glob.glob(dirpath + '/*')\n    for path in filepaths:\n        with open(path, 'rb') as f:\n            byte_content = f.read()\n            str_content = byte_content.decode('utf-8', errors='ignore')\n            files.append(str_content)\n    return files",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00050-880791e3-fdca-4d6d-8806-004f4d8e152c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a71038da",
    "execution_start": 1630635717127,
    "execution_millis": 1370,
    "deepnote_cell_type": "code"
   },
   "source": "# load the datasets\nspam = load_dataset(spam_dir)\neasy_ham = load_dataset(easy_ham_dir)\nhard_ham = load_dataset(hard_ham_dir)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00051-f1ab33fc-7776-462a-bb6f-498154972723",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "99158df9",
    "execution_start": 1630635718504,
    "execution_millis": 499,
    "deepnote_cell_type": "code"
   },
   "source": "import sklearn.utils\nX = spam + easy_ham +hard_ham\ny = np.concatenate((np.ones(len(spam)), np.zeros(len(easy_ham)+len(hard_ham))))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00052-df5395a4-6e69-46fa-a776-fdbc670e769f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2834f551",
    "execution_start": 1630635719008,
    "execution_millis": 111772,
    "deepnote_cell_type": "code"
   },
   "source": "features_matrix = np.zeros((len(X),1899))\nfor i in range(len(X)):\n    word_indices = processEmail(X[i], verbose=False)\n    features_matrix[i] = emailFeatures(word_indices)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00053-dce1c363-2bf3-4501-a970-b67531362acc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6d106d7a",
    "execution_start": 1630635830805,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "source": "print(features_matrix)\nprint(features_matrix.shape)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n(3046, 1899)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00054-37a3ec9f-c917-4d37-8c4c-b4596f6334c3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d17e4c4c",
    "execution_start": 1630635830806,
    "execution_millis": 8,
    "deepnote_cell_type": "code"
   },
   "source": "print(y)\nprint(y.shape)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "[1. 1. 1. ... 0. 0. 0.]\n(3046,)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00052-11b76a51-6424-4709-b887-6ba6245c9344",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7115d430",
    "execution_start": 1630635830807,
    "execution_millis": 100,
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.model_selection import train_test_split\n\n# shuffle the dataset\nfeatures_matrix, y = sklearn.utils.shuffle(features_matrix, y, random_state=42)\n\n# split the data into stratified training and test sets\nX_train, X_test, y_train, y_test = train_test_split(features_matrix, y, test_size=0.2, stratify=y,\n                                                    random_state=42)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00054-b2489ade-72bb-4a0e-899b-766cd9012ecc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d0a7b7d6",
    "execution_start": 1630635830913,
    "execution_millis": 1086,
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn import svm\nfrom sklearn import metrics\n\nclf = svm.SVC(kernel='linear')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n# Model Precision: what percentage of positive tuples are labeled as such?\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Accuracy: 0.9868852459016394\nPrecision: 0.9927536231884058\nRecall: 0.9785714285714285\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-1b100df8-9751-4a04-8cbe-86136dc4f44b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "baad7d30",
    "execution_start": 1630635832003,
    "execution_millis": 3183,
    "deepnote_cell_type": "code"
   },
   "source": "clf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n# Model Precision: what percentage of positive tuples are labeled as such?\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Accuracy: 0.9836065573770492\nPrecision: 0.9927007299270073\nRecall: 0.9714285714285714\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=9b4ecd43-3a8f-4a83-95be-819561460e5a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "deepnote_notebook_id": "e6ce26fd-250d-4fab-a65d-fecf277a44fb",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}